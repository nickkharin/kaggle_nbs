{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import  metrics\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('exoTrain.csv')\n",
    "y_train = X_train['LABEL']\n",
    "X_train = X_train.drop(columns=['LABEL'], axis=1)\n",
    "X_test =pd.read_csv('exoTest.csv')\n",
    "y_test = X_test['LABEL']\n",
    "X_test = X_test.drop(columns=['LABEL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>FLUX.10</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93.85</td>\n",
       "      <td>83.81</td>\n",
       "      <td>20.10</td>\n",
       "      <td>-26.98</td>\n",
       "      <td>-39.56</td>\n",
       "      <td>-124.71</td>\n",
       "      <td>-135.18</td>\n",
       "      <td>-96.27</td>\n",
       "      <td>-79.89</td>\n",
       "      <td>-160.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-78.07</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>25.13</td>\n",
       "      <td>48.57</td>\n",
       "      <td>92.54</td>\n",
       "      <td>39.32</td>\n",
       "      <td>61.42</td>\n",
       "      <td>5.08</td>\n",
       "      <td>-39.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38.88</td>\n",
       "      <td>-33.83</td>\n",
       "      <td>-58.54</td>\n",
       "      <td>-40.09</td>\n",
       "      <td>-79.31</td>\n",
       "      <td>-72.81</td>\n",
       "      <td>-86.55</td>\n",
       "      <td>-85.33</td>\n",
       "      <td>-83.97</td>\n",
       "      <td>-73.38</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-24.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-11.70</td>\n",
       "      <td>6.46</td>\n",
       "      <td>16.00</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>532.64</td>\n",
       "      <td>535.92</td>\n",
       "      <td>513.73</td>\n",
       "      <td>496.92</td>\n",
       "      <td>456.45</td>\n",
       "      <td>466.00</td>\n",
       "      <td>464.50</td>\n",
       "      <td>486.39</td>\n",
       "      <td>436.56</td>\n",
       "      <td>484.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>13.31</td>\n",
       "      <td>13.31</td>\n",
       "      <td>-29.89</td>\n",
       "      <td>-20.88</td>\n",
       "      <td>5.06</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>-28.91</td>\n",
       "      <td>-70.02</td>\n",
       "      <td>-96.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326.52</td>\n",
       "      <td>347.39</td>\n",
       "      <td>302.35</td>\n",
       "      <td>298.13</td>\n",
       "      <td>317.74</td>\n",
       "      <td>312.70</td>\n",
       "      <td>322.33</td>\n",
       "      <td>311.31</td>\n",
       "      <td>312.42</td>\n",
       "      <td>323.33</td>\n",
       "      <td>...</td>\n",
       "      <td>5.71</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>30.05</td>\n",
       "      <td>20.03</td>\n",
       "      <td>-12.67</td>\n",
       "      <td>-8.77</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-17.35</td>\n",
       "      <td>13.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1107.21</td>\n",
       "      <td>-1112.59</td>\n",
       "      <td>-1118.95</td>\n",
       "      <td>-1095.10</td>\n",
       "      <td>-1057.55</td>\n",
       "      <td>-1034.48</td>\n",
       "      <td>-998.34</td>\n",
       "      <td>-1022.71</td>\n",
       "      <td>-989.57</td>\n",
       "      <td>-970.88</td>\n",
       "      <td>...</td>\n",
       "      <td>-594.37</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-357.24</td>\n",
       "      <td>-443.76</td>\n",
       "      <td>-438.54</td>\n",
       "      <td>-399.71</td>\n",
       "      <td>-384.65</td>\n",
       "      <td>-411.79</td>\n",
       "      <td>-510.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7   FLUX.8  \\\n",
       "0    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   -96.27   \n",
       "1   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   -85.33   \n",
       "2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   486.39   \n",
       "3   326.52   347.39   302.35   298.13   317.74   312.70  322.33   311.31   \n",
       "4 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34 -1022.71   \n",
       "\n",
       "   FLUX.9  FLUX.10  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0  -79.89  -160.17  ...     -78.07    -102.15    -102.15      25.13   \n",
       "1  -83.97   -73.38  ...      -3.28     -32.21     -32.21     -24.89   \n",
       "2  436.56   484.39  ...     -71.69      13.31      13.31     -29.89   \n",
       "3  312.42   323.33  ...       5.71      -3.73      -3.73      30.05   \n",
       "4 -989.57  -970.88  ...    -594.37    -401.66    -401.66    -357.24   \n",
       "\n",
       "   FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0      48.57      92.54      39.32      61.42       5.08     -39.54  \n",
       "1      -4.86       0.76     -11.70       6.46      16.00      19.93  \n",
       "2     -20.88       5.06     -11.80     -28.91     -70.02     -96.67  \n",
       "3      20.03     -12.67      -8.77     -17.31     -17.35      13.98  \n",
       "4    -443.76    -438.54    -399.71    -384.65    -411.79    -510.54  \n",
       "\n",
       "[5 rows x 3197 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose a robust scaler for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = RobustScaler()\n",
    "scl.fit(X_train)\n",
    "X_train_scl = scl.transform(X_train)\n",
    "scl.fit(X_test)\n",
    "X_test_scl = scl.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting train score: 1.0\n",
      "Gradient Boosting test score: 0.987719298245614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      0.99       565\n",
      "           2       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.99       570\n",
      "   macro avg       0.50      0.50      0.50       570\n",
      "weighted avg       0.98      0.99      0.99       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GB = GradientBoostingClassifier()\n",
    "GB.fit(X_train_scl, y_train)\n",
    "prediction_GB=GB.predict(X_test_scl)\n",
    "train_score_GB = GB.score(X_train_scl, y_train)\n",
    "test_score_GB = GB.score(X_test_scl, y_test)\n",
    "print(f\"Gradient Boosting train score: {train_score_GB}\")\n",
    "print(f\"Gradient Boosting test score: {test_score_GB}\")\n",
    "print(classification_report(y_test, prediction_GB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree train score: 1.0\n",
      "Decision Tree test score: 0.9842105263157894\n",
      "Decision Tree Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.99      0.99       565\n",
      "           2       0.25      0.40      0.31         5\n",
      "\n",
      "    accuracy                           0.98       570\n",
      "   macro avg       0.62      0.69      0.65       570\n",
      "weighted avg       0.99      0.98      0.99       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train_scl, y_train)\n",
    "prediction_DT=DT.predict(X_test_scl)\n",
    "train_score_DT = DT.score(X_train_scl, y_train)\n",
    "test_score_DT = DT.score(X_test_scl, y_test)\n",
    "print(f\"Decision Tree train score: {train_score_DT}\")\n",
    "print(f\"Decision Tree test score: {test_score_DT}\")\n",
    "print('Decision Tree Classifier')\n",
    "print(classification_report(y_test, prediction_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost train score: 1.0\n",
      "Adaboost test score: 0.9912280701754386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      1.00       565\n",
      "           2       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.99       570\n",
      "   macro avg       0.50      0.50      0.50       570\n",
      "weighted avg       0.98      0.99      0.99       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "AB = AdaBoostClassifier()\n",
    "AB.fit(X_train_scl, y_train)\n",
    "prediction_AB=AB.predict(X_test_scl)\n",
    "train_score_AB = AB.score(X_train_scl, y_train)\n",
    "test_score_AB = AB.score(X_test_scl, y_test)\n",
    "print(f\"AdaBoost train score: {train_score_AB}\")\n",
    "print(f\"Adaboost test score: {test_score_AB}\")\n",
    "print(classification_report(y_test, prediction_AB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF train score: 1.0\n",
      "RF test score: 0.9912280701754386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      1.00       565\n",
      "           2       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.99       570\n",
      "   macro avg       0.50      0.50      0.50       570\n",
      "weighted avg       0.98      0.99      0.99       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train_scl, y_train)\n",
    "prediction_RF=RF.predict(X_test_scl)\n",
    "train_score_RF = RF.score(X_train_scl, y_train)\n",
    "test_score_RF = RF.score(X_test_scl, y_test)\n",
    "print(f\"RF train score: {train_score_RF}\")\n",
    "print(f\"RF test score: {test_score_RF}\")\n",
    "print(classification_report(y_test, prediction_RF))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG train score: 0.997444466286613\n",
      "BG test score: 0.9912280701754386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      1.00       565\n",
      "           2       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.99       570\n",
      "   macro avg       0.50      0.50      0.50       570\n",
      "weighted avg       0.98      0.99      0.99       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "BG = BaggingClassifier()\n",
    "BG.fit(X_train, y_train)\n",
    "prediction_BG=BG.predict(X_test)\n",
    "train_score_BG = BG.score(X_train, y_train)\n",
    "test_score_BG = BG.score(X_test, y_test)\n",
    "print(f\"BG train score: {train_score_BG}\")\n",
    "print(f\"BG test score: {test_score_BG}\")\n",
    "print(classification_report(y_test, prediction_BG))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used some kinds of models: Gradient Boosting, Decision Tree, AdaBoost, Random Forest and Bagging. I chose them for better understanding of how they work. As we can see on the diagram, they perform good results. \n",
    "I will try some experiments with another models soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'Gradient Boosting': {'Train': train_score_GB, 'Test': test_score_GB},\n",
    "        'Decision Tree': {'Train': train_score_DT, 'Test': test_score_DT},\n",
    "        'AdaBoost': {'Train': train_score_AB, 'Test': test_score_AB},\n",
    "        'Random Forest': {'Train': train_score_RF, 'Test': test_score_RF},\n",
    "        'Bagging': {'Train': train_score_BG, 'Test': test_score_BG}}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.T\n",
    "df ['sum'] = df.sum(axis=1)\n",
    "df.sort_values('sum', ascending=False)[['Test','Train']].plot.bar() \n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
